{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Music Recommendation System</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h3>Group Members</h3></u>\n",
    "Shartil Shartilov, 22085025<br>\n",
    "Riaz Khan, 23068308<br>\n",
    "Najeeb Ullah, 23061856\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pip Install Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install shapely\n",
    "%pip install node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.prepared import prep\n",
    "from shapely.geometry import mapping, shape, Point\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Const Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column const values\n",
    "YEAR_COLUMN = \"year\"\n",
    "TEMPO_COLUMN = \"tempo\"\n",
    "LOUDNESS_COLUMN = \"loudness\"\n",
    "DURATION_COLUMN = \"duration\"\n",
    "SONG_HOTTTNESSS_COLUMN = \"song_hotttnesss\"\n",
    "ARTIST_HOTTTNESSS_COLUMN = \"artist_hotttnesss\"\n",
    "ARTIST_FAMILIARITY_COLUMN = \"artist_familiarity\"\n",
    "DECADE_COLUMN = \"decade\"\n",
    "\n",
    "# numeric columns name list\n",
    "NUMERIC_COLUMNS_LIST = [\n",
    "    YEAR_COLUMN,\n",
    "    TEMPO_COLUMN,\n",
    "    LOUDNESS_COLUMN,\n",
    "    DURATION_COLUMN,\n",
    "    SONG_HOTTTNESSS_COLUMN,\n",
    "    ARTIST_HOTTTNESSS_COLUMN,\n",
    "    ARTIST_FAMILIARITY_COLUMN,\n",
    "    DECADE_COLUMN\n",
    "]\n",
    "\n",
    "# string column names\n",
    "SONG_TITLE_COLUMN = \"song_title\"\n",
    "COUNTRY_COLUMN = \"country\"\n",
    "\n",
    "# artist location columns \n",
    "ARTIST_LONGITUDE_COLUMN = \"artist_longitude\"\n",
    "ARTIST_LATITUDE_COLUMN = \"artist_latitude\"\n",
    "ARTIST_LOCATION_COLUMN = \"artist_location\"\n",
    "UNKNOWN_COUNTRY_VALUE = \"unknown\"\n",
    "\n",
    "# id columns \n",
    "ARTIST_ID_COLUMN = \"artist_id\"\n",
    "SONG_ID_COLUMN = \"song_id\"\n",
    "\n",
    "# const path values\n",
    "MUSIC_DATA_FOLDER_PATH = \"../Music Data/\"\n",
    "MODELS_FOLDER_PATH = \"../models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_node_name(node_type, node_value):\n",
    "    \"This method returns concatenated string of node type and node value\"\n",
    "    return f\"{node_type} {node_value}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(lon, lat, countries_dict):\n",
    "    \"This method returns country name based on latitude and longitude values\"\n",
    "    point = Point(lon, lat)\n",
    "    for country, geom in countries_dict.items():\n",
    "        if geom.contains(point):\n",
    "            return country\n",
    "\n",
    "    return UNKNOWN_COUNTRY_VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load Songs & Artists datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_songs_dataset = pd.read_csv(\"../Data/songs_dataset.csv\")\n",
    "raw_artists_dataset = pd.read_csv(\"../Data/artist_terms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging datasets based on artist_id<br>\n",
    "In the following cell, duplicate rows based on `artist_id` will be removed and only the first value will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the artist dataset based on artist_id\n",
    "raw_artists_dataset = raw_artists_dataset.drop_duplicates(subset=ARTIST_ID_COLUMN, keep='first')\n",
    "\n",
    "# Merge the datasets on artist_id\n",
    "raw_music_dataset = pd.merge(raw_songs_dataset, raw_artists_dataset, on=ARTIST_ID_COLUMN, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, the datasets were merged based on artist_id using a left join.<br>\n",
    "This join means that all the keys from the left dataframe (in this case, the raw_songs_dataset dataframe) will be included in the merged dataframe, and only the matching keys from the right dataframe (in this case, the artist_dataset dataframe) will be added.\n",
    "<br><br>\n",
    "In other words, all rows from the left dataframe (raw_songs_dataset) are retained.<br>\n",
    "If there are matching keys (in this case, artist_id) in the right dataframe (artist_dataset), the corresponding data from the right dataframe will be added to the merged dataframe.\n",
    "If there are no matching keys in the right dataframe, the corresponding columns in the merged dataframe will be filled with NaN (missing values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the merged raw dataframe at first glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_music_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_music_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_music_dataset.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the raw dataframe contains plenty of rows with missing data.<br>\n",
    "These rows will be deleted, since we require rows with complete data for our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_dataset = raw_music_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(music_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system does not require a vast amount of songs.<br>\n",
    "Instead we will select a small portion of over 1000 songs to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(music_dataset.shape)\n",
    "\n",
    "# returns dataframe with (1058, 13)\n",
    "music_dataset = music_dataset.iloc[::120]\n",
    "\n",
    "print(music_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's introduce a new column \"country\" based on Latitude and Longitude of each artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below is used to load geographical data from a local GeoJSON file, process it, and subsequently determine which country a given set of latitude and logitude coordinates falls into. The country names extracted from GeoJSON file are then inserted into a new column in a dataset.\n",
    "\n",
    "- The \"json.load\" function reads the file and convert it into a Python dictionary ('geojson_data').\n",
    "\n",
    "- An empty dictionary named 'countries' is initiated to store the coordinated data associated with each country.\n",
    "\n",
    "- The script iterates over each feature in the 'features' array of the 'geojson_data'. Each features represents a country.\n",
    "\n",
    "- For each feature, the geometry ('geom') and the administrative name of the country is extracted.\n",
    "\n",
    "- The geometry is then processed with a function 'prep' applied to 'shape(geom)'. This likely involves creating a geometric shape from the geometry data and preparing it for fast spatial queries. The processed geometry is stored in the 'countries' dictionary with the country name as key.\n",
    "\n",
    "- A function 'get_country' is defined which takes longitudes ('lon') and latitude ('lat') as arguments and creates a ('Point') object from coordinates.\n",
    "\n",
    "- It then iterates  over the 'countries' dictionary and check whether the point is contained within any of the country geometrics using the 'contains' method of the geometry.\n",
    "\n",
    "- If a containing country is found, the function returns the country's name. if no containing country is found, it returns a value UNKNOWN_COUNTRY_VALUE\n",
    "- A new column in the dataset ('music_dataset') is populated by applying 'get_country' function to each row. In this way the country column is added to music_dataset based on latitude and logitude columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch and process the geojson data from a local file\n",
    "with open(r'../Data/countries.geojson.json', 'r') as file:\n",
    "    geojson_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill countries dict with data\n",
    "countries_dict = {}\n",
    "for feature in geojson_data[\"features\"]:\n",
    "    geom = feature[\"geometry\"]\n",
    "    country = feature[\"properties\"][\"ADMIN\"]\n",
    "    countries_dict[country] = prep(shape(geom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to create a new 'country' column\n",
    "music_dataset[COUNTRY_COLUMN] = music_dataset.apply(\n",
    "    lambda row: get_country(\n",
    "        row[ARTIST_LONGITUDE_COLUMN], \n",
    "        row[ARTIST_LATITUDE_COLUMN],\n",
    "        countries_dict), \n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's delete redundant columns in the music dataframe and songs with unknown coutry value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_dataset = music_dataset.drop(\n",
    "    [\n",
    "        ARTIST_LATITUDE_COLUMN,\n",
    "        ARTIST_LONGITUDE_COLUMN,\n",
    "        ARTIST_LOCATION_COLUMN,\n",
    "        SONG_ID_COLUMN,\n",
    "        ARTIST_ID_COLUMN\n",
    "    ], \n",
    "    axis=1)\n",
    "\n",
    "\n",
    "music_dataset = music_dataset[music_dataset[COUNTRY_COLUMN] != UNKNOWN_COUNTRY_VALUE]\n",
    "music_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a final batch of 1000 songs to use in our recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dataframe with 1000 songs\n",
    "music_dataset = music_dataset.iloc[:1000]\n",
    "\n",
    "print(music_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expend the music dataset by adding a decade column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_dataset[YEAR_COLUMN] = music_dataset[YEAR_COLUMN].astype(int)\n",
    "music_dataset = music_dataset.assign(decade=lambda row: (row[YEAR_COLUMN].astype(int) // 10) * 10)\n",
    "music_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather decade data for the knowledge graph creation in the upcoming steps\n",
    "min_decade = music_dataset[DECADE_COLUMN].min()\n",
    "max_decade = music_dataset[DECADE_COLUMN].max()\n",
    "decade_array = np.arange(min_decade, max_decade + 10, 10, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the processed dataset to a CSV file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MUSIC_DATA_FOLDER_PATH):\n",
    "    os.mkdir(MUSIC_DATA_FOLDER_PATH)\n",
    "\n",
    "music_dataset.to_csv(f\"{MUSIC_DATA_FOLDER_PATH}/music_dataset.csv\", mode='w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Graph Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize_column(df, column_name):\n",
    "    \"This method performs min max standardisation\"\n",
    "    min_val = df[column_name].min()\n",
    "    max_val = df[column_name].max()\n",
    "    \n",
    "    if min_val == max_val:\n",
    "        raise ValueError(\"Cannot normalize column when all values are the same.\")\n",
    "    \n",
    "    df[column_name] = (df[column_name] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's standardise the numeric columns in the music dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_music_dataset = music_dataset.copy()\n",
    "\n",
    "for numeric_column in NUMERIC_COLUMNS_LIST:\n",
    "    min_max_normalize_column(normalized_music_dataset, numeric_column)\n",
    "\n",
    "normalized_music_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standardising the data, we can create the knowledge graph for our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_index, current_row in normalized_music_dataset.iterrows():\n",
    "    # creating a data dict that contains numeric data for each song node\n",
    "    node_data_dict = {}\n",
    "\n",
    "    for current_column in NUMERIC_COLUMNS_LIST:\n",
    "        # filling the data dict with the different numeric values for each song node\n",
    "        node_data_dict[current_column] = current_row[current_column]\n",
    "\n",
    "    # adding a song node to the graph with the numeric data\n",
    "    music_graph.add_node(str(current_index), **node_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding empty nodes for the decades, which function as main nodes\n",
    "# i.e., all the songs from 1950 will be connected to the 1950 node.\n",
    "# This will save complex logic of connecting all the songs from the decade, and ensuring that the resulted graph will be less complicated.\n",
    "for current_decade in decade_array:\n",
    "    node_name = get_attribute_node_name(DECADE_COLUMN, current_decade)\n",
    "    music_graph.add_node(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting between the song nodes and the main decade nodes\n",
    "for index, row in music_dataset.iterrows():\n",
    "    current_decade = row[DECADE_COLUMN]\n",
    "    node_name = get_attribute_node_name(DECADE_COLUMN, current_decade)\n",
    "    music_graph.add_edge(node_name, str(index))\n",
    "\n",
    "print(music_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the created Knowledge graph.<br>\n",
    "Please note that this code was taken from the code notebook from week 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spring layout\n",
    "pos = nx.spring_layout(music_graph)\n",
    "\n",
    "# compute degree centrality\n",
    "# degree represents the number of edges from each node,\n",
    "# the centrality allows us to undertsand the more 'popular' nodes\n",
    "cent = nx.degree_centrality(music_graph)\n",
    "cent_array = np.array(list(cent.values()))\n",
    "\n",
    "# size of nodes will be proportional to their popularity\n",
    "node_size = list(map(lambda x: x * 500, cent.values()))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "#draw nodes\n",
    "nodes = nx.draw_networkx_nodes(music_graph, pos, node_size=node_size,\n",
    "                               cmap=plt.cm.plasma,\n",
    "                               nodelist=list(cent.keys()))\n",
    "# draw edges\n",
    "edges = nx.draw_networkx_edges(music_graph, pos, width=0.25, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Embedding and KNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's embed the song nodes using Node2Vec algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "node2vec = Node2Vec(music_graph, dimensions=64, walk_length=10, num_walks=100, workers=4)\n",
    "\n",
    "# embed graph nodes\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings for future use\n",
    "model.wv.save_word2vec_format(f\"{MODELS_FOLDER_PATH}/embedding\")\n",
    "\n",
    "# save model for future use\n",
    "model.save(f\"{MODELS_FOLDER_PATH}/node2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train an unsupervised version of the KNN classifier using the received embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(music_graph.nodes())\n",
    "embeddings = np.array([model.wv[str(node)] for node in nodes])\n",
    "nearestNeighborSelector = NearestNeighbors(n_neighbors=6).fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_id(song_title):\n",
    "    \"This method returns music dataset ID of a given song name\"\n",
    "    matching_songs = music_dataset[music_dataset[SONG_TITLE_COLUMN].str.lower().str.contains(song_title.lower())]\n",
    "    if not matching_songs.empty:\n",
    "        return matching_songs.iloc[0].name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs(query):\n",
    "    \"This method displays song recommendations based on given song name\"\n",
    "    user_song_title = query\n",
    "    song_id = get_song_id(user_song_title)\n",
    "\n",
    "    if song_id is not None:\n",
    "        user_song_embed = model.wv[str(song_id)]\n",
    "        distances, indices = nearestNeighborSelector.kneighbors([user_song_embed])\n",
    "        print(\"Similar songs:\")\n",
    "        similar_songs_df = music_dataset.loc[indices[0]]\n",
    "        display(similar_songs_df)\n",
    "    else:\n",
    "        print(\"Song not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_by_query(query_type, query):\n",
    "    \"This method displays song recommendations based on given query type and query value\"\n",
    "    filtered_music_df = music_dataset[music_dataset[query_type].astype(str).str.lower() == str(query).lower()]\n",
    "    \n",
    "    if len(filtered_music_df) > 0:\n",
    "        chosen_song = filtered_music_df.sample(n=1)\n",
    "        recommend_songs(chosen_song[SONG_TITLE_COLUMN].iloc[0])\n",
    "    else:\n",
    "        print(\"No songs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's setup the user interface of the system and start using it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_options_dict = {\n",
    "    1 : [\"song_title\", \"song title\"],\n",
    "    2 : [\"artist_name\", \"artist name\"],\n",
    "    3 : [\"decade\", \"decade\"],\n",
    "    4 : [\"country\", \"country\"],\n",
    "    5 : [\"term (genre)\", \"term\"],\n",
    "    6 : [\"year\", \"year\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Query by: song_title, artist_name, decade, country, term (genre) or year\")\n",
    "    input_text = \"Enter your query type (provide number only):\"\n",
    "\n",
    "    for user_option_id, user_option in user_options_dict.items():\n",
    "        input_text += f\"\\n{user_option_id} -> {user_option[0]}\"\n",
    "\n",
    "    user_query_input = int(input(input_text))\n",
    "    query_type = user_options_dict[user_query_input][0]\n",
    "    query = input(f\"Enter value for the {user_options_dict[user_query_input][1]}\")\n",
    "\n",
    "    if query_type == 'song_title':\n",
    "        recommend_songs(query)\n",
    "    else:\n",
    "        recommend_songs_by_query(query_type, query)\n",
    "except ValueError as ex:\n",
    "    print('Please provide valid input')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a5cfde8991b0f64e8bcd60a397bea8dc10ed042aefe1441fd3daa2ae2091421"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
